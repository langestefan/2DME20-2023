{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Optimization of a quadratic function\n",
    "\n",
    "#### Problem definition\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &= \\frac{1}{2} x^T A x\\\\\n",
    "A &= diag(1 \\dots 1000)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Our starting point is given to be: $x_0 = [1000, 999, \\dots, 1]^T$\n",
    "\n",
    "We are constrained to use steepest descent and to terminate when we satisfy the following condition:\n",
    "\n",
    "$$\n",
    "||\\nabla f(x_k)||_2 \\leq 10^{-6} ||\\nabla f(x_0)||_2\n",
    "$$\n",
    "\n",
    "Our goal is to minimize the number of function evalutions $f(x_k)$ by optimizing the step size $\\alpha_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 shape:  (1000,)\n",
      "A shape:  (1000, 1000)\n",
      "f(x0) shape:  ()\n",
      "df(x0) shape:  (1000,)\n",
      "tol:  5.787947275744656\n"
     ]
    }
   ],
   "source": [
    "# define the constants\n",
    "A = np.diag(np.arange(1, 1001))\n",
    "x0 = np.arange(1000, 0, -1)\n",
    "print(\"x0 shape: \", x0.shape)\n",
    "print(\"A shape: \", A.shape) \n",
    "\n",
    "# define the function\n",
    "def Q(x):\n",
    "    return 0.5 * x.T @ A @ x\n",
    "print(\"f(x0) shape: \", Q(x0).shape)\n",
    "\n",
    "# define the gradient\n",
    "def dQ(x):\n",
    "    return A @ x\n",
    "print(\"df(x0) shape: \", dQ(x0).shape)\n",
    "\n",
    "# compute 2 norm\n",
    "def norm(x):\n",
    "    return np.sqrt(x.T @ x)\n",
    "\n",
    "tol = 1e-6 * norm(dQ(x0))\n",
    "print(\"tol: \", tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Steepest descent\n",
    "\n",
    "The general idea behind iterative methods is to compute a step along search direction $d_k$ and update the current point $x_k$ by adding the step $d_k$ to it. The history of steepest descent is very well explained in [1].\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{k+1} &= x_k + \\alpha_k d_k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the method of steepest descent the direction is given by the negative gradient of the function $f$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d_k &= -\\nabla f(x_k)\\\\\n",
    "x_{k+1} &= x_k - \\alpha_k \\nabla f(x_k)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For our quadratic model $f(x) = \\frac{1}{2} x^T A x$ we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla f(x) &= Ax\\\\\n",
    "&= diag(1 \\dots 1000) x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{k+1} &= x_k - \\alpha_k A x_k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Step size optimization\n",
    "\n",
    "We can optimize the step size $\\alpha_k$ by solving the following optimization problem, which only makes sense for convex $f$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_k &= \\arg\\min_{\\alpha} f(x_k - \\alpha \\nabla f(x_k))\\\\\n",
    "         &= \\arg\\min_{\\alpha} f(x_k - \\alpha A x_k)\\\\\n",
    "         &= \\arg\\min_{\\alpha} \\frac{1}{2} (x_k - \\alpha A x_k)^T A (x_k - \\alpha A x_k)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_k &= \\frac{d_k^T d_k}{d_k^T A d_k}\\\\\n",
    "         &= \\frac{x_k^T A^2 x_k}{x_k^T A^3 x_k}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Meza, J. C. (2010). Steepest descent. Wiley Interdisciplinary Reviews: Computational Statistics, 2(6), 719-722."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive implementation\n",
    "\n",
    "We take the most simple solution, where $\\alpha_k = c > 0$ is a small constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Optim:\n",
    "    \"\"\"Generic implementation of an optimization algorithm class.\"\"\"\n",
    "    def __init__(self, x0: np.ndarray, f: Callable, new_alpha: Callable, df: Callable, tol: float):\n",
    "        \"\"\"\n",
    "        :param x0: initial guess\n",
    "        :param f: objective function\n",
    "        :param new_alpha: update rule to obtain new alpha_k\n",
    "        :param df: gradient of f\n",
    "        :param tol: tolerance for stopping criterion\n",
    "        \"\"\"\n",
    "    \n",
    "        self.x = x0\n",
    "        self.f = f\n",
    "        self.new_alpha = new_alpha\n",
    "        self.df = df\n",
    "        self.tol = tol\n",
    "        self.fs = [f(x0)]\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Optimize the objective function.\"\"\"\n",
    "        # starting point\n",
    "        x_k = self.x\n",
    "        alpha_k = self.new_alpha(x_k)\n",
    "\n",
    "        # iterate until stopping criterion is satisfied\n",
    "        idx = 0\n",
    "        norm_df_xk = np.inf\n",
    "\n",
    "        while norm_df_xk > self.tol:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # update x_k and alpha_k\n",
    "            x_k = x_k - alpha_k * self.df(x_k)\n",
    "            alpha_k = self.new_alpha(x_k)\n",
    "\n",
    "            # store the function value\n",
    "            f_xk = self.f(x_k)\n",
    "            norm_df_xk = norm(self.df(x_k))\n",
    "            self.fs.append(f_xk)\n",
    "\n",
    "            # print the iteration\n",
    "            display(f\"Iteration {idx}: f(x) = {f_xk:.2f}, ||df(x)|| = {norm_df_xk:.2f}\")\n",
    "            idx += 1\n",
    "\n",
    "        self.x = x_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 5149: f(x) = 16.73, ||df(x)|| = 5.78'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 5151\n"
     ]
    }
   ],
   "source": [
    "naive_optim = Optim(x0, Q, lambda x: 1e-3, dQ, tol)\n",
    "naive_optim.optimize()\n",
    "print(f\"Number of function evaluations: {len(naive_optim.fs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal implementation, see Eq. (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 2749: f(x) = 8.36, ||df(x)|| = 5.78'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 2751\n"
     ]
    }
   ],
   "source": [
    "optimal_alpha_k = lambda x: (x.T @ A @ A @ x) / (x.T @ A @ A @ A @ x)\n",
    "optimal_optim = Optim(x0, Q, optimal_alpha_k, dQ, tol)\n",
    "optimal_optim.optimize()\n",
    "print(f\"Number of function evaluations: {len(optimal_optim.fs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line search with Armijo condition\n",
    "\n",
    "The Armijo condition is given by, see [2] Eq. 3.4:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_k + \\alpha_k d_k) &\\leq f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^T d_k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $c_1 \\in (0, 1)$ is a small constant. We can use the Armijo condition to find a step size $\\alpha_k$ that satisfies the condition. We start with $\\alpha_k = 1$ and decrease it by a factor of $0.5$ until the condition is satisfied.\n",
    "\n",
    "[2] Jorge, N., & Stephen, J. W. (2006). Numerical optimization. Spinger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 2714: f(x) = 7.92, ||df(x)|| = 5.76'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 2716\n"
     ]
    }
   ],
   "source": [
    "# define update by Armijo rule\n",
    "def armijo_alpha_k(x_k, alpha=1, beta=0.5, c_1=0.01):\n",
    "    dQ_xk = dQ(x_k)\n",
    "    dQ_xk_T = np.transpose(dQ_xk)\n",
    "    while Q(x_k - alpha * dQ_xk) > Q(x_k) - c_1 * alpha * dQ_xk_T @ dQ_xk:\n",
    "        alpha *= beta\n",
    "    return alpha\n",
    "\n",
    "armijo_optim = Optim(x0, Q, armijo_alpha_k, dQ, tol)\n",
    "armijo_optim.optimize()\n",
    "print(f\"Number of function evaluations: {len(armijo_optim.fs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line search with Armijo + curvature conditions (Wolfe conditions)\n",
    "\n",
    "The Wolfe conditions are given by, see [2] Eq. 3.6a and 3.6b:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_k + \\alpha_k d_k) &\\leq f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^T d_k\\\\\n",
    "\\nabla f(x_k + \\alpha_k d_k)^T d_k &\\geq c_2 \\nabla f(x_k)^T d_k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $c_1 \\in (0, 1)$ and $c_2 \\in (c_1, 1)$ are small constants. We can use the Wolfe conditions to find a step size $\\alpha_k$ that satisfies the conditions. We start with $\\alpha_k = 1$ and decrease it by a factor of $0.5$ until the conditions are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 1291: f(x) = 14.76, ||df(x)|| = 5.62'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 1293\n"
     ]
    }
   ],
   "source": [
    "# define update rule with Wolfe conditions\n",
    "def wolfe_alpha_k(x_k, alpha=1, beta=0.5, c_1=0.5, c_2=0.9):\n",
    "    dQ_xk = dQ(x_k)\n",
    "    dQ_xk_T = np.transpose(dQ_xk)\n",
    "    while Q(x_k - alpha * dQ_xk) > Q(x_k) - c_1 * alpha * dQ_xk_T @ dQ_xk and \\\n",
    "          dQ(x_k - alpha * dQ_xk) @ dQ_xk < c_2 * dQ_xk_T @ dQ_xk:\n",
    "        alpha *= beta\n",
    "    return alpha\n",
    "\n",
    "wolfe_optim = Optim(x0, Q, wolfe_alpha_k, dQ, tol)\n",
    "wolfe_optim.optimize()\n",
    "print(f\"Number of function evaluations: {len(wolfe_optim.fs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barzilai and Borwein method\n",
    "\n",
    "The Barzilai and Borwein method is given by, see [1] Eq. 13 and 14:\n",
    "\n",
    "We write the iterate update rule as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{k+1} &= x_k - \\frac{1}{\\alpha_k}  d_k\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We then compute the step length according to the formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_k &= \\frac{s_{k-1}^T y_{k-1}}{s_{k-1}^T s0_{k-1}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $s_{k-1} = x_k - x_{k-1}$ and $y_{k-1} = d_k - d_{k-1}$.\n",
    "\n",
    "[1] Barzilai, J., & Borwein, J. M. (1988). Two-point step size gradient methods. IMA journal of numerical analysis, 8(1), 141-148."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barzilai and borwein method\n",
    "def barzilai_borwein_alpha_k(x_k, alpha=1):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
